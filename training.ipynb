{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run clusters through three different classification algorithms, choose whichever performs the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import matthews_corrcoef as mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data_dict(file):\n",
    "    fnames = glob(os.path.join('data',file,'*.csv'))\n",
    "    data = {name.split('/')[-1][:-4]: pd.read_csv(name, index_col='Id') for name in fnames}\n",
    "    #some of the clusters don't have any failuers, can't model without both classes\n",
    "    dfs = [d for d in data.keys() if data[d]['Response'].sum()>1]\n",
    "    return data, dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_class_weights(data, dfs):\n",
    "    class_weights = {}\n",
    "    for df in dfs:\n",
    "        dfx = data[df]\n",
    "        n_failures = dfx.Response.sum()\n",
    "        class_weights[df] = n_failures/dfx.shape[0]\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_training(data, dfs):\n",
    "    X_trains, X_tests, y_trains, y_tests = {},{},{},{}\n",
    "    for df in dfs:\n",
    "        X_trains[df], X_tests[df], y_trains[df], y_tests[df] = \\\n",
    "        train_test_split(data[df].drop('Response', axis=1).values, \\\n",
    "                         data[df]['Response'], \\\n",
    "                         stratify=data[df]['Response'])\n",
    "    return X_trains, X_tests, y_trains, y_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_classifiers(dfs, class_weights, X_trains, X_tests, y_trains, y_tests, munge):\n",
    "    lrs = {df: LogisticRegression(fit_intercept=True).fit(X_trains[df], y_trains[df]) for df in dfs}\n",
    "    rfs = {df: RandomForestClassifier(max_features='sqrt',\\\n",
    "                                      class_weight={0:1-class_weights[df],\\\n",
    "                                      1:class_weights[df]}).fit(X_trains[df], y_trains[df])\\\n",
    "                                      for df in dfs}\n",
    "    dts = {df: DecisionTreeClassifier(max_features='sqrt',\\\n",
    "                                      class_weight={0:1-class_weights[df],\\\n",
    "                                      1:class_weights[df]}).fit(X_trains[df], y_trains[df])\\\n",
    "                                      for df in dfs}\n",
    "    \n",
    "    classifiers = {munge:{'lrs': lrs, 'rfs': rfs, 'dts': dts}}\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def make_preds(dfs, classifiers, X_tests, munge):\n",
    "    d_preds = {df: {} for df in dfs}\n",
    "    for df in dfs:\n",
    "        for name, classifier in classifiers[munge].items():\n",
    "            d_preds[df][name] = classifier[df].predict(X_tests[df])\n",
    "    return d_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(dfs, d_preds, y_tests):\n",
    "    d_scores = {df: {} for df in dfs} \n",
    "    for df in dfs:\n",
    "        for name, preds in d_preds[df].items():\n",
    "            d_scores[df][name] = mc(y_tests[df], preds)\n",
    "    return d_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_classifier(d_scores, dfs):\n",
    "    d_choice = {}\n",
    "    for df in dfs:\n",
    "        best = -1\n",
    "        for name, score in d_scores[df].items():\n",
    "            if score > best:\n",
    "                best = score\n",
    "                choice = name\n",
    "        d_choice[df] = choice\n",
    "    return d_choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(munges):\n",
    "    '''\n",
    "    INPUT: lst of directories containing .csv files, \n",
    "    each entry cooresponding to a different munge\n",
    "    OUTPUT: lst of dicts\n",
    "    '''\n",
    "    result = []\n",
    "    models = {}\n",
    "    for munge in munges:\n",
    "        data, dfs = make_data_dict(munge)\n",
    "        class_weights = get_class_weights(data, dfs)\n",
    "        X_trains, X_tests, y_trains, y_tests = split_training(data, dfs)\n",
    "        classifiers = train_classifiers(dfs, class_weights, X_trains, X_tests, y_trains, y_tests, munge)\n",
    "        models[list(classifiers.keys())[0]] = classifiers[munge]\n",
    "        d_preds = make_preds(dfs, classifiers[munge], X_tests)\n",
    "        d_scores = score(dfs, d_preds, y_tests)\n",
    "        d_choice = choose_classifier(d_scores, dfs)\n",
    "        scores = {df: d_scores[df][d_choice[df]] for df in dfs}\n",
    "        for df in dfs:\n",
    "            d = {df:{}}\n",
    "            d[df]['score'] = d_scores[df][d_choice[df]]\n",
    "            d[df]['classifier'] = d_choice[df]\n",
    "            d[df]['munge'] = munge    \n",
    "            result.append(d)\n",
    "    return result, dfs, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    '''\n",
    "    instead of 'decomp' being boolean, use the \n",
    "    filename as a value of 'munge' also use filename to keep track of models\n",
    "    '''\n",
    "    final = {}\n",
    "    result, dfs, models = get_result(['pca', 'clusters'])\n",
    "    hi_scores = {df:0.0 for df in dfs}\n",
    "    for df in dfs:\n",
    "        for d in result:\n",
    "            dfx = list(d.keys())[0]\n",
    "            if  dfx == df and d['score'] > hi_scores[dfx]:\n",
    "                hi_scores[dfx] = d['score']\n",
    "                final[dfx] = {'classifier': d[dfx]['classifier'], 'munge':d[dfx]['munge']}\n",
    "    for name, d in final.items():\n",
    "        with open(os.path.join('..','data','models', munge, name), 'wb') as f:\n",
    "            pickle.dump(models[d['classifier']], f)\n",
    "    with open(os.path.join('..','data','models','modelchoices.json'), 'wb') as f:\n",
    "        pickle.dump(final, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryanbusby/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_classifiers() missing 1 required positional argument: 'file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-157c9bda2cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-f3a05ae2857e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     '''\n\u001b[1;32m      6\u001b[0m     \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clusters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mhi_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-ccde7f4ddf3d>\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_class_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mX_trains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tests\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mclassifiers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_classifiers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_trains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tests\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifiers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0md_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_tests\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: train_classifiers() missing 1 required positional argument: 'file'"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
